package tests

//
//import (
//	//"github.com/aws/aws-sdk-go/aws"
//	//"github.com/aws/aws-sdk-go/service/s3"
//
//	//"github.com/portworx/torpedo/drivers/scheduler"
//	"github.com/portworx/torpedo/pkg/s3utils"
//
//	. "github.com/onsi/ginkgo"
//
//	"github.com/portworx/torpedo/pkg/log"
//	//"github.com/portworx/torpedo/drivers/scheduler"
//	. "github.com/portworx/torpedo/tests"
//)
//
//var _ = Describe("{sseS3encryption}", func() {
//
//	var (
//		//scheduledAppContexts     []*scheduler.Context
//		customBucket string
//		//backupLocationUID string
//		//cloudCredUID      string
//		//backupName               string
//		//cloudCredUidList         []string
//		//customBackupLocationName string
//		//credName                 string
//		customBuckets []string
//		//bucketNames   []string
//		//clusterUid               string
//		//bkpNamespaces            []string
//		//clusterStatus            api.ClusterInfo_StatusInfo_Status
//	)
//
//	Step("Adding Credentials and Registering Backup Location", func() {
//		providers := getProviders()
//		log.InfoD("Using pre-provisioned bucket. Creating cloud credentials and backup location.")
//		//ctx, err := backup.GetAdminCtxFromSecret()
//		//log.FailOnError(err, "Fetching px-central-admin ctx")
//		// Create a bucket name prefix
//		bucketMap := map[string]bool{
//			"sse-bucket-with-policy-1":  true,
//			"sse-bucket-with-policy-2":  true,
//			"sse-bucket-without-policy": false,
//		}
//
//		// Create the buckets without policy
//		for _, provider := range providers {
//			for bucketName, policy := range bucketMap {
//				customBucket = GetCustomBucketName(provider, bucketName)
//				if policy {
//					sseDetails, err := s3utils.GetS3SSEDetailsFromEnv()
//					log.FailOnError(err, "Failed to generate s3 bucket policy check for the correctness of policy parameters")
//					policy, err := GenerateS3BucketPolicy(sseDetails.SsePolicySid, string(sseDetails.SseEncryptionPolicy), customBucket)
//					log.FailOnError(err, "Failed to generate s3 bucket policy check for the correctness of policy parameters")
//					err = UpdateS3BucketPolicy(customBucket, policy)
//					log.FailOnError(err, "Failed to apply bucket policy")
//					log.Infof("Updated S3 bucket policy - %s", customBucket)
//				}
//				customBuckets = append(customBuckets, customBucket)
//			}
//		}
//
//		// Create the buckets with policy
//
//		//for _, provider := range providers {
//		//	for _, customBucket := range bucketNames {
//		//cloudCredUID = uuid.New()
//		//cloudCredUidList = append(cloudCredUidList, cloudCredUID)
//		//backupLocationUID = uuid.New()
//		//credName = fmt.Sprintf("autogenerated-cred-%v", time.Now().Unix())
//		//err := CreateCloudCredential(provider, credName, cloudCredUID, orgID, ctx)
//		//dash.VerifyFatal(err, nil, fmt.Sprintf("Verifying creation of cloud credential named [%s] for org [%s] with [%s] as provider", credName, orgID, provider))
//		//log.InfoD("Created Cloud Credentials with name - %s", credName)
//		//customBackupLocationName = fmt.Sprintf("autogenerated-backup-location-%v", time.Now().Unix())
//		//customBucket = GetCustomBucketName(provider, customBucket)
//		//err = CreateBackupLocation(provider, customBackupLocationName, backupLocationUID, credName, cloudCredUID, customBucket, orgID, "")
//		//dash.VerifyFatal(err, nil, fmt.Sprintf("Creating backup location %s", customBackupLocationName))
//		//log.InfoD("Created Backup Location with name - %s", customBackupLocationName)
//		//	}
//		//}
//	})
//
//	//JustAfterEach(func() {
//	//	// Post test custom bucket delete
//	//	providers := getProviders()
//	//	for _, provider := range providers {
//	//		for _, customBucket := range bucketNames {
//	//			DeleteBucket(provider, customBucket)
//	//			customBuckets = append(customBuckets, customBucket)
//	//		}
//	//	}
//	//	defer EndPxBackupTorpedoTest(make([]*scheduler.Context, 0))
//	//})
//
//})

//	for _, provider := range providers {
//		for bucketName, policy := range bucketMap {
//			customBucket = GetCustomBucketName(provider, bucketName)
//			if policy {
//				//sseDetails, err := s3utils.GetS3SSEDetailsFromEnv()
//				//log.FailOnError(err, "Failed to generate s3 bucket policy check for the correctness of policy parameters")
//				policy, err := GenerateS3BucketPolicy("DenyNonAES256Uploads", string(api.S3Config_SSE_S3), customBucket)
//				log.FailOnError(err, "Failed to generate s3 bucket policy check for the correctness of policy parameters")
//				err = UpdateS3BucketPolicy(customBucket, policy)
//				log.FailOnError(err, "Failed to apply bucket policy")
//				log.Infof("Updated S3 bucket policy - %s", globalAWSBucketName)
//			}
//			customBuckets = append(customBuckets, customBucket)
//		}
//	}
//
//	//ModifyS3BucketWithKmsEncryption("aws-sse-bucket-without-policy-1697607465")
//	//log.InfoD("KMS policy updated successfully")
//
//	//// Create backup locations
//	for _, provider := range providers {
//		for _, customBucket := range customBuckets {
//			cloudCredUID = uuid.New()
//			cloudCredUidList = append(cloudCredUidList, cloudCredUID)
//			backupLocationUID = uuid.New()
//			credName = fmt.Sprintf("autogenerated-cred-%v", time.Now().Unix())
//			err := CreateCloudCredential(provider, credName, cloudCredUID, orgID, ctx)
//			dash.VerifyFatal(err, nil, fmt.Sprintf("Verifying creation of cloud credential named [%s] for org [%s] with [%s] as provider", credName, orgID, provider))
//			log.InfoD("Created Cloud Credentials with name - %s", credName)
//			customBackupLocationName = fmt.Sprintf("autogenerated-backup-location-%v", time.Now().Unix())
//			//err = CreateS3BackupLocationWithSseType(customBackupLocationName, backupLocationUID, credName, cloudCredUID, customBucket, orgID, "", api.S3Config_SSE_S3)
//			err = CreateBackupLocation(provider, customBackupLocationName, backupLocationUID, credName, cloudCredUID, customBucket, orgID, "")
//			dash.VerifyFatal(err, nil, fmt.Sprintf("Creating backup location %s", customBackupLocationName))
//			log.Infof("created backup location successfully")
//			time.Sleep(60 * time.Second)
//			err = UpdateBackupLocation(provider, ctx, api.S3Config_SSE_S3)
//			log.InfoD("Created Backup Location with name - %s", customBackupLocationName)
//			log.Infof("updated backup location successfully")
//			//backupLocations = append(backupLocations, customBackupLocationName)
//			//log.InfoD("Taking backup of application for different combination of restores")
//			//backupName = fmt.Sprintf("%s-%s-%v", BackupNamePrefix, bkpNamespaces[0], time.Now().Unix())
//			//appContextsToBackup := FilterAppContextsByNamespace(scheduledAppContexts, []string{bkpNamespaces[0]})
//			//err = CreateBackupWithValidation(ctx, backupName, SourceClusterName, customBackupLocationName, backupLocationUID, appContextsToBackup, make(map[string]string), orgID, clusterUid, "", "", "", "")
//			//dash.VerifyFatal(err, nil, fmt.Sprintf("Creation and Validation of backup [%s]", backupName))
//			//backupNames = append(backupNames, backupName)
//		}
//	}
//})
//Step("Taking backup of application for different combination of restores", func() {
//	log.InfoD("Taking backup of application for different combination of restores")
//	//for _, namespace := range bkpNamespaces {
//	//for _, bkpLocationName := range backupLocations {
//	//	backupName = fmt.Sprintf("%s-%s-%v", BackupNamePrefix, bkpNamespaces[0], time.Now().Unix())
//	//	appContextsToBackup := FilterAppContextsByNamespace(scheduledAppContexts, []string{bkpNamespaces[0]})
//	//	err = CreateBackupWithValidation(ctx, backupName, SourceClusterName, bkpLocationName, backupLocationUID, appContextsToBackup, make(map[string]string), orgID, clusterUid, "", "", "", "")
//	//	backupNames = append(backupNames, backupName)
//	//	dash.VerifyFatal(err, nil, fmt.Sprintf("Creation and Validation of backup [%s]", backupName))
//	//}
//	//}
//	//Step("Taking backup of application for different combination of restores", func() {
//	//	log.InfoD("Taking backup of application for different combination of restores")
//	//	backupName = fmt.Sprintf("%s-%s-%v", BackupNamePrefix, bkpNamespaces[0], time.Now().Unix())
//	//	appContextsToBackup := FilterAppContextsByNamespace(scheduledAppContexts, []string{bkpNamespaces[0]})
//	//	err = CreateBackupWithValidation(ctx, backupName, SourceClusterName, bkpLocationName, backupLocationUID, appContextsToBackup, make(map[string]string), orgID, clusterUid, "", "", "", "")
//	//	dash.VerifyFatal(err, nil, fmt.Sprintf("Creation and Validation of backup [%s]", backupName))
//	//})
//})
//Step("Create new storage class on source cluster for storage class mapping for restore", func() {
//	log.InfoD("Create new storage class on source cluster for storage class mapping for restore")
//	scCount = 3
//	for i := 0; i < scCount; i++ {
//		scName := fmt.Sprintf("replica-sc-%d-%v", time.Now().Unix(), i)
//		params["repl"] = "2"
//		v1obj := metaV1.ObjectMeta{
//			Name: scName,
//		}
//		reclaimPolicyDelete := v1.PersistentVolumeReclaimDelete
//		bindMode := storageApi.VolumeBindingImmediate
//		scObj := storageApi.StorageClass{
//			ObjectMeta:        v1obj,
//			Provisioner:       k8s.CsiProvisioner,
//			Parameters:        params,
//			ReclaimPolicy:     &reclaimPolicyDelete,
//			VolumeBindingMode: &bindMode,
//		}
//
//		_, err := k8sStorage.CreateStorageClass(&scObj)
//		dash.VerifyFatal(err, nil, fmt.Sprintf("Creating new storage class %v on source cluster %s", scName, SourceClusterName))
//		scNames = append(scNames, scName)
//	}
//})
//Step("Multiple restore for same backup in different storage class in same cluster at the same time", func() {
//	log.InfoD(fmt.Sprintf("Multiple restore for same backup into %d different storage class in same cluster at the same time", scCount))
//	ctx, err := backup.GetAdminCtxFromSecret()
//	log.FailOnError(err, "Fetching px-central-admin ctx")
//	pvcs, err := core.Instance().GetPersistentVolumeClaims(bkpNamespaces[0], make(map[string]string))
//	singlePvc := pvcs.Items[0]
//	sourceScName, err = core.Instance().GetStorageClassForPVC(&singlePvc)
//	var wg sync.WaitGroup
//	//for _, scName := range scNames {
//	for i := 0; i < 1; i++ {
//		storageClassMapping[sourceScName.Name] = scNames[i]
//		time.Sleep(2)
//		namespaceMap[bkpNamespaces[0]] = fmt.Sprintf("new-namespace-%v", time.Now().Unix())
//		restoreName := fmt.Sprintf("restore-new-storage-class-%s-%s", scNames[i], RestoreNamePrefix)
//		restoreList = append(restoreList, restoreName)
//		wg.Add(1)
//		go func(scName string) {
//			defer GinkgoRecover()
//			defer wg.Done()
//			err = CreateRestore(restoreName, backupNames[0], namespaceMap, SourceClusterName, orgID, ctx, storageClassMapping)
//			dash.VerifyFatal(err, nil, fmt.Sprintf("Restoring backup %v using storage class %v", backupName, scName))
//		}(scNames[i])
//	}
//	wg.Wait()
//})
//Step("Multiple restore for same backup in different storage class in different cluster at the same time", func() {
//	log.InfoD(fmt.Sprintf("Multiple restore for same backup into %d different storage class in same cluster at the same time", scCount))
//	ctx, err := backup.GetAdminCtxFromSecret()
//	log.FailOnError(err, "Fetching px-central-admin ctx")
//	pvcs, err := core.Instance().GetPersistentVolumeClaims(bkpNamespaces[0], make(map[string]string))
//	singlePvc := pvcs.Items[0]
//	sourceScName, err = core.Instance().GetStorageClassForPVC(&singlePvc)
//	var wg sync.WaitGroup
//	//for _, scName := range scNames {
//	for i := 2; i < 3; i++ {
//		storageClassMapping[sourceScName.Name] = scNames[i]
//		time.Sleep(2)
//		namespaceMap[bkpNamespaces[0]] = fmt.Sprintf("new-namespace-%v", time.Now().Unix())
//		restoreName := fmt.Sprintf("restore-new-storage-class-%s-%s", scNames[i], RestoreNamePrefix)
//		restoreList = append(restoreList, restoreName)
//		wg.Add(1)
//		go func(scName string) {
//			defer GinkgoRecover()
//			defer wg.Done()
//			err = CreateRestore(restoreName, backupNames[1], namespaceMap, destinationClusterName, orgID, ctx, storageClassMapping)
//			dash.VerifyFatal(err, nil, fmt.Sprintf("Restoring backup %v using storage class %v", backupName, scName))
//		}(scNames[i])
//	}
//	wg.Wait()
//})

//JustAfterEach(func() {
//	// Post test custom bucket delete
//	providers := getProviders()
//	time.Sleep(100 * time.Second)
//	for _, provider := range providers {
//		for _, customBucket := range bucketNames {
//			DeleteBucket(provider, customBucket)
//			customBuckets = append(customBuckets, customBucket)
//		}
//	}
//	defer EndPxBackupTorpedoTest(make([]*scheduler.Context, 0))
